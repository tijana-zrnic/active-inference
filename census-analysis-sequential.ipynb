{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6e339f-560e-4aed-b5b1-9c961e7d5b35",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb227a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import matplotlib.patheffects as pe\n",
    "from utils import ols, make_width_coverage_plot, make_budget_plot, get_data, transform_features\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ded62-56bd-4221-be01-8502a7010df0",
   "metadata": {},
   "source": [
    "### Import the ACS PUMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a312e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['AGEP','SCHL','MAR','DIS','ESP','CIT','MIG','MIL','ANC1P','NATIVITY','DEAR','DEYE','DREM','SEX','RAC1P', 'SOCP', 'COW']\n",
    "ft = np.array([\"q\", \"q\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"])\n",
    "income_features, income, employed = get_data(year=2019, features=features, outcome='PINCP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5086f2-6f7d-4b32-ae4a-412338e0c783",
   "metadata": {},
   "source": [
    "### Problem setup\n",
    "\n",
    "Split data into labeled and unlabeled subsets. Compute ground-truth value of the regression coefficient. Specify range of budgets in fractional form $n_b/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25badf26-0184-45e4-b46e-ba83c8d5307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = len(income)\n",
    "n_tr = 100\n",
    "\n",
    "income_features_labeled, income_features_unlabeled, income_labeled, Y = train_test_split(income_features, income, train_size=n_tr)\n",
    "income_labeled = income_labeled.to_numpy()\n",
    "\n",
    "X = np.stack([income_features_unlabeled['AGEP'].to_numpy(), income_features_unlabeled['SEX'].to_numpy()], axis=1)\n",
    "Y = Y.to_numpy()\n",
    "age = income_features['AGEP'].to_numpy()\n",
    "sex = income_features['SEX'].to_numpy()\n",
    "theta_true = ols(np.stack([age, sex], axis=1), income.to_numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2aafa-2e14-4c94-9309-ae5467bee3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "budgets = np.linspace(0.01, 0.03, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65990d46-1fc4-4b57-874a-01e368fe23ca",
   "metadata": {},
   "source": [
    "### Train XGBoost model\n",
    "\n",
    "Train XGBoost model on labeled data. Additionally train auxiliary model for predicting the magnitude of prediction error. Compute model uncertainty for unlabeled instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3bcb-21ef-4dd4-9064-ab0d3ccece29",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_features_enc, enc = transform_features(income_features, ft)\n",
    "income_features_labeled = transform_features(income_features_labeled, ft, enc)[0]\n",
    "income_features_unlabeled = transform_features(income_features_unlabeled, ft, enc)[0]\n",
    "\n",
    "dtrain = xgb.DMatrix(income_features_labeled, label=income_labeled)\n",
    "tree = xgb.train({'eta': 0.3, 'max_depth': 7, 'objective': 'reg:absoluteerror'}, dtrain, 100)\n",
    "Yhat = tree.predict(xgb.DMatrix(income_features_unlabeled))\n",
    "\n",
    "dtrain = xgb.DMatrix(income_features_labeled, label=np.abs(income_labeled - tree.predict(xgb.DMatrix(income_features_labeled))))\n",
    "tree_err = xgb.train({'eta': 0.3, 'max_depth': 7, 'objective': 'reg:absoluteerror'}, dtrain, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d1042-6be3-4912-a9ef-138c60f7c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hessian_inv = np.linalg.inv(1/X.shape[0] * X.T @ X)\n",
    "h = Hessian_inv[:,0]\n",
    "predicted_errs_init = np.clip(tree_err.predict(xgb.DMatrix(income_features_unlabeled)), 0, np.inf)\n",
    "uncertainty = np.abs(h.dot(X.T)) * predicted_errs_init\n",
    "C_init = np.mean(uncertainty)\n",
    "C = C_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5752c-78cd-4f01-bd43-aa7f32236d54",
   "metadata": {},
   "source": [
    "### Main experiment\n",
    "\n",
    "Forms dataframe ```df``` with experiment results. The columns in the dataframe are:\n",
    "\n",
    "- ```lb``` - interval lower bound\n",
    "\n",
    "- ```ub``` - interval upper bound\n",
    "\n",
    "- ```interval width``` - equal to ```ub``` - ```lb```\n",
    "\n",
    "- ```coverage``` - 0/1 indicator of whether or not interval covered target\n",
    "\n",
    "- ```estimator``` - one of ```active (w/ fine-tuning)```, ```uniform```, or ```active (no fine-tuning)```\n",
    "\n",
    "- ```$n_b$``` - budget size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68269e6b-5afb-4a65-aad9-8bb373c90f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_all - n_tr\n",
    "num_trials = 100\n",
    "alpha = 0.1\n",
    "tau = 0.5\n",
    "\n",
    "# fine-tuning params\n",
    "batch_size = 1000\n",
    "steps = 200\n",
    "eta = 0.01\n",
    "greedy_steps = 500\n",
    "steps_err = 500\n",
    "eta_err = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607cf00-0d17-46ab-a24b-4b5335a539cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "columns = [\"lb\", \"ub\", \"interval width\", \"coverage\", \"estimator\", \"$n_b$\"]\n",
    "temp_df = pd.DataFrame(np.zeros((3,len(columns))), columns=columns)\n",
    "\n",
    "for j in tqdm(range(len(budgets))):\n",
    "    budget = budgets[j]\n",
    "    budget_window = int(greedy_steps/budget) # how often we use up remaing budget\n",
    "\n",
    "    for k in tqdm(range(num_trials)):\n",
    "        perm = np.random.choice(range(n), n, replace=False)\n",
    "        Y = Y[perm]\n",
    "        Yhat = Yhat[perm]\n",
    "        X = X[perm]\n",
    "        uncertainty = uncertainty[perm]\n",
    "        income_features_unlabeled = income_features_unlabeled[perm]\n",
    "        predicted_errs_init = predicted_errs_init[perm]\n",
    "        increments_active = []\n",
    "        increments_nofinetune = []\n",
    "        finetune_inds = []\n",
    "        num_collected_active = 0\n",
    "        num_collected_nofinetune = 0\n",
    "        tree_new = tree.copy()\n",
    "        tree_err_new = tree_err.copy()\n",
    "        Yhat_new = tree_new.predict(xgb.DMatrix(income_features_unlabeled))\n",
    "        predicted_errs = predicted_errs_init\n",
    "        \n",
    "        for i in range(n):\n",
    "            Yhat_curr = Yhat_new[i]\n",
    "            uncertainty_normed_curr = np.abs(h.dot(X[i])) * predicted_errs[i] / C * budget\n",
    "            raw_prob = np.clip(uncertainty_normed_curr, 0, np.maximum(0,(i+1)*budget - num_collected_active))\n",
    "            if i % budget_window >= budget_window - greedy_steps:\n",
    "                raw_prob = (i+1)*budget - num_collected_active\n",
    "            prob = (1-tau)*np.clip(raw_prob, 0, 1) + tau*budget\n",
    "            xi = bernoulli.rvs(prob)\n",
    "            if xi == 1:\n",
    "                finetune_inds.append(i)\n",
    "                num_collected_active +=1\n",
    "            increments_active.append(float(Yhat_curr + (Y[i] - Yhat_curr)*xi/prob))\n",
    "                \n",
    "            if len(finetune_inds) == batch_size:\n",
    "                finetune_data = xgb.DMatrix(income_features_unlabeled[finetune_inds], label=Y[finetune_inds])\n",
    "                tree_new = xgb.train({'eta': eta, 'max_depth': 7, 'objective': 'reg:absoluteerror'}, finetune_data, steps, xgb_model=tree_new)\n",
    "                errs = np.abs(Y[finetune_inds] - tree_new.predict(xgb.DMatrix(income_features_unlabeled[finetune_inds])))\n",
    "                finetune_data_err = xgb.DMatrix(income_features_unlabeled[finetune_inds], label=errs)\n",
    "                tree_err_new = xgb.train({'eta': eta_err, 'max_depth': 7, 'objective': 'reg:absoluteerror'}, finetune_data_err, steps_err, xgb_model=tree_err_new)\n",
    "                predicted_errs = np.clip(tree_err_new.predict(xgb.DMatrix(income_features_unlabeled)), 0, np.inf)\n",
    "                C = np.mean(np.abs(h.dot(X.T)) * predicted_errs)\n",
    "                Yhat_new = tree_new.predict(xgb.DMatrix(income_features_unlabeled))\n",
    "                finetune_inds = []\n",
    " \n",
    "            raw_prob_nofinetune = np.clip(uncertainty[i] / C_init * budget, 0, np.maximum(0,(i+1)*budget - num_collected_nofinetune))\n",
    "            if i % budget_window >= budget_window - greedy_steps:\n",
    "                raw_prob_nofinetune = (i+1)*budget - num_collected_nofinetune\n",
    "            prob_nofinetune = (1-tau)*np.clip(raw_prob_nofinetune, 0, 1) + tau*budget\n",
    "\n",
    "            # couple sampling decisions to minimize variance in results\n",
    "            if prob_nofinetune > prob:\n",
    "                if xi == 1:\n",
    "                    xi_nofinetune = 1\n",
    "                else:\n",
    "                    xi_nofinetune = bernoulli.rvs((prob_nofinetune - prob)/(1-prob))\n",
    "            else:\n",
    "                if xi == 0:\n",
    "                    xi_nofinetune = 0\n",
    "                else:\n",
    "                    xi_nofinetune = bernoulli.rvs(prob_nofinetune/prob)\n",
    "            \n",
    "            if xi_nofinetune == 1:\n",
    "                num_collected_nofinetune += 1\n",
    "                \n",
    "            increments_nofinetune.append(Yhat[i] + (Y[i] - Yhat[i])*xi_nofinetune/prob_nofinetune)\n",
    "\n",
    "        active_labels = np.array(increments_active)\n",
    "        pointest_active = ols(X, active_labels)\n",
    "        grads = np.zeros(X.shape)\n",
    "        for i in range(n):\n",
    "            grads[i,:] = (np.dot(X[i,:], pointest_active) - active_labels[i]) * X[i,:]\n",
    "        V = np.cov(grads.T)\n",
    "        Sigma_active = Hessian_inv @ V @ Hessian_inv\n",
    "        pointest_active_std = np.sqrt(Sigma_active[0,0])/np.sqrt(n)\n",
    "        width_active = norm.ppf(1-alpha/2)*pointest_active_std \n",
    "        coverage_active = (theta_true >= pointest_active[0] - width_active)*(theta_true <= pointest_active[0] + width_active)   \n",
    "        temp_df.loc[0] = pointest_active[0] - width_active, pointest_active[0] + width_active, 2*width_active, coverage_active, \"active (w/ fine-tuning)\", int(budget*n)\n",
    "        \n",
    "        nofinetune_labels = np.array(increments_nofinetune)\n",
    "        pointest_nofinetune = ols(X, nofinetune_labels)\n",
    "        grads = np.zeros(X.shape)\n",
    "        for i in range(n):\n",
    "            grads[i,:] = (np.dot(X[i,:], pointest_nofinetune) - nofinetune_labels[i]) * X[i,:]\n",
    "        V = np.cov(grads.T)\n",
    "        Sigma_nofinetune = Hessian_inv @ V @ Hessian_inv\n",
    "        pointest_nofinetune_std = np.sqrt(Sigma_nofinetune[0,0])/np.sqrt(n)\n",
    "        width_nofinetune = norm.ppf(1-alpha/2)*pointest_nofinetune_std \n",
    "        coverage_nofinetune = (theta_true >= pointest_nofinetune[0] - width_nofinetune)*(theta_true <= pointest_nofinetune[0] + width_nofinetune)   \n",
    "        temp_df.loc[1] = pointest_nofinetune[0] - width_nofinetune, pointest_nofinetune[0] + width_nofinetune, 2*width_nofinetune, coverage_nofinetune, \"active (no fine-tuning)\", int(budget*n)\n",
    "        \n",
    "        xi_unif = bernoulli.rvs([budget]*n)\n",
    "        unif_labels = np.array(Yhat + (Y - Yhat)*xi_unif/budget)\n",
    "        pointest_unif = ols(X, unif_labels)\n",
    "        grads = np.zeros(X.shape)\n",
    "        for i in range(n):\n",
    "            grads[i,:] = (np.dot(X[i,:], pointest_unif) - unif_labels[i]) * X[i,:]\n",
    "        V = np.cov(grads.T)\n",
    "        Sigma_unif = Hessian_inv @ V @ Hessian_inv\n",
    "        pointest_unif_std = np.sqrt(Sigma_unif[0,0])/np.sqrt(n)\n",
    "        width_unif = norm.ppf(1-alpha/2)*pointest_unif_std \n",
    "        coverage_unif = (theta_true >= pointest_unif[0] - width_unif)*(theta_true <= pointest_unif[0] + width_unif)\n",
    "        temp_df.loc[2] = pointest_unif[0] - width_unif, pointest_unif[0] + width_unif, 2*width_unif, coverage_unif, \"uniform\", int(budget*n)\n",
    "        \n",
    "        results += [temp_df.copy()]\n",
    "df = pd.concat(results,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee350e4-fe79-41e8-a153-58c1b4d5117a",
   "metadata": {},
   "source": [
    "### Plot coverage and interval width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dafc15-f14d-4d83-9f19-f9d2cdf7d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_width_coverage_plot(df, \"regression coefficient\", \"widths_and_coverage_census_seq.pdf\", theta_true, num_trials=num_trials, n_example_ind=3, finetuning=True, less_precision=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8fb10-2b61-4b49-876f-81c65aa8e700",
   "metadata": {},
   "source": [
    "### Plot budget saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f3072-9295-49e5-abde-bcf7ca8b0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_budget_plot(df, \"Census data analysis\", \"budget_census_seq.pdf\", finetuning=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
